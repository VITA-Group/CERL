{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2221\n",
      "2221\n",
      "2221\n",
      "BATCHNORM:  False\n",
      "Epoch 0 Begin\n",
      "Epoch 0, Iter 0, Loss: 2.3578\n",
      "Loss1: 0.2042, Loss2: 1.9583, Loss4: 0.1953\n",
      "5\n",
      "Epoch 0, Iter 1, Loss: 2.2932\n",
      "Loss1: 0.1607, Loss2: 1.9799, Loss4: 0.1527\n",
      "Epoch 0, Iter 2, Loss: 2.4449\n",
      "Loss1: 0.1328, Loss2: 2.1670, Loss4: 0.1451\n",
      "Epoch 0, Iter 3, Loss: 1.6442\n",
      "Loss1: 0.1256, Loss2: 1.4237, Loss4: 0.0949\n",
      "Epoch 0, Iter 4, Loss: 1.2587\n",
      "Loss1: 0.0582, Loss2: 1.0645, Loss4: 0.1360\n",
      "Epoch 0, Iter 5, Loss: 0.9164\n",
      "Loss1: 0.0457, Loss2: 0.7370, Loss4: 0.1337\n",
      "Epoch 0, Iter 6, Loss: 0.8369\n",
      "Loss1: 0.1774, Loss2: 0.5740, Loss4: 0.0855\n",
      "Epoch 0, Iter 7, Loss: 1.0213\n",
      "Loss1: 0.0695, Loss2: 0.8593, Loss4: 0.0925\n",
      "Epoch 0, Iter 8, Loss: 0.5766\n",
      "Loss1: 0.0310, Loss2: 0.4924, Loss4: 0.0532\n",
      "Epoch 0, Iter 9, Loss: 1.2054\n",
      "Loss1: 0.0450, Loss2: 1.0966, Loss4: 0.0638\n",
      "Epoch 0, Iter 10, Loss: 2.8594\n",
      "Loss1: 0.0704, Loss2: 2.6836, Loss4: 0.1053\n",
      "9\n",
      "Epoch 0, Iter 11, Loss: 0.7924\n",
      "Loss1: 0.0419, Loss2: 0.6663, Loss4: 0.0841\n",
      "Epoch 0, Iter 12, Loss: 0.6632\n",
      "Loss1: 0.0299, Loss2: 0.5825, Loss4: 0.0508\n",
      "Epoch 0, Iter 13, Loss: 1.4011\n",
      "Loss1: 0.0517, Loss2: 1.2871, Loss4: 0.0623\n",
      "Epoch 0, Iter 14, Loss: 0.5894\n",
      "Loss1: 0.0227, Loss2: 0.4737, Loss4: 0.0930\n",
      "Epoch 0, Iter 15, Loss: 0.8462\n",
      "Loss1: 0.0513, Loss2: 0.7490, Loss4: 0.0458\n",
      "Epoch 0, Iter 16, Loss: 0.7219\n",
      "Loss1: 0.0639, Loss2: 0.6124, Loss4: 0.0455\n",
      "Epoch 0, Iter 17, Loss: 1.2648\n",
      "Loss1: 0.0403, Loss2: 1.0454, Loss4: 0.1792\n",
      "Epoch 0, Iter 18, Loss: 0.9349\n",
      "Loss1: 0.0516, Loss2: 0.7876, Loss4: 0.0956\n",
      "Epoch 0, Iter 19, Loss: 0.5698\n",
      "Loss1: 0.0482, Loss2: 0.4813, Loss4: 0.0403\n",
      "Epoch 0, Iter 20, Loss: 0.7851\n",
      "Loss1: 0.0171, Loss2: 0.6129, Loss4: 0.1551\n",
      "21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f81af923ff54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mtest_images_outp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mtest_images_outp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mtest_images_outp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/code/FLAVR-main/myutils.py\u001b[0m in \u001b[0;36mtest_images_outp\u001b[0;34m(model, device, time, epoch, iter_id)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    459\u001b[0m     \"\"\"\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "from PIL import Image\n",
    "# from generate_video import generate_video\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import vgg16\n",
    "from perceptual import LossNetwork\n",
    "import random\n",
    "\n",
    "import config\n",
    "import myutils\n",
    "from myutils import test_images_3d, test_images_2d, test_images_outp, test_metric, adjust_learning_rate, convert_to_gray, get_raft_args\n",
    "from loss import Loss\n",
    "from loss import edge_conv2d\n",
    "from torch.utils.data import DataLoader\n",
    "from loss import L1_Charbonnier_loss\n",
    "from pdb import set_trace as bp\n",
    "\n",
    "import models\n",
    "from utils import make_coord\n",
    "from torch.autograd import Variable\n",
    "from loss import PerceptualLoss, DCLoss\n",
    "\n",
    "from core.raft import RAFT\n",
    "from core.utils import flow_viz\n",
    "from core.utils.utils import InputPadder\n",
    "\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "from dataset.GoPro_arbitrary_nosr import make_coord_3d\n",
    "\n",
    "\n",
    "def double_forward(model, optimizer, preds, images, gt, device, i, epoch_id, bs):\n",
    "    \n",
    "    sampled_idx = sorted(random.sample(range(5), 3))\n",
    "    h, w = preds[0].shape[2], preds[0].shape[3]\n",
    "    for idx in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        temp_coord = make_coord_3d((h, w), (idx + 2) / 8)\n",
    "        temp_coord = [temp_coord.to(device)[None].repeat(bs, 1, 1)]\n",
    "        if idx == 0:\n",
    "            inputs = torch.cat([images[:, :, 1].unsqueeze(0), preds[1].detach().unsqueeze(0), preds[3].detach().unsqueeze(0), preds[5].detach().unsqueeze(0)], dim=0).permute(1, 2, 0, 3, 4)\n",
    "        elif idx == 1:\n",
    "            inputs = torch.cat([preds[0].detach().unsqueeze(0), preds[2].detach().unsqueeze(0), preds[4].detach().unsqueeze(0), preds[6].detach().unsqueeze(0)], dim=0).permute(1, 2, 0, 3, 4)\n",
    "        else:\n",
    "            inputs = torch.cat([preds[1].detach().unsqueeze(0), preds[3].detach().unsqueeze(0), preds[5].detach().unsqueeze(0), images[:, :, 2].unsqueeze(0)], dim=0).permute(1, 2, 0, 3, 4)\n",
    "        new_pred_f, new_pred_b = model(inputs, temp_coord, True)\n",
    "        loss = F.smooth_l1_loss(new_pred_f[0], gt[idx + 2])\\\n",
    "            + F.smooth_l1_loss(new_pred_b[0], gt[idx + 2])\\\n",
    "            + F.smooth_l1_loss(new_pred_b[0], new_pred_f[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch %d, Iter %d, Loss: %.4f' % (epoch_id, i, loss.item()))\n",
    "        \n",
    "    return \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##### Tensorboard #####\n",
    "writer = SummaryWriter('/output/logs')\n",
    "\n",
    "##### Parameters #####\n",
    "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--data_root', type=str, default='/data/nnice1216/vimeo_septuplet/DAVIS/JPEGImages/Full-Resolution/bmx-rider/')\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--lr', type=float, default=5e-5)\n",
    "parser.add_argument('--epoch_num', type=int, default=30)\n",
    "parser.add_argument('--num_workers', type=int, default=8)\n",
    "parser.add_argument('--if_continue', type=bool, default=False)\n",
    "parser.add_argument('--TEMP', type=float, default=1)\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "##### Dataset ###### \n",
    "## DAVIS \n",
    "# from dataset.Davis_liif import get_loader\n",
    "# args.data_root = '/data/nnice1216/vimeo_septuplet/DAVIS/JPEGImages/Full-Resolution/bmx-rider/' \n",
    "# train_loader = get_loader(args.data_root, args.batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "\n",
    "\n",
    "## VIMEO ##\n",
    "# from dataset.vimeo90k_septuplet import get_loader\n",
    "# args.data_root = '/data/nnice1216/vimeo_septuplet/'\n",
    "# train_loader = get_loader('train', args.data_root, args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "\n",
    "\n",
    "## GOPRO ##\n",
    "from dataset.GoPro_arbitrary_nosr import get_loader\n",
    "random_seed = 0\n",
    "interval = 5\n",
    "# train_data_root = '/data/nnice1216/X4K1000FPS_dataset/train/'\n",
    "train_data_root = '/data/nnice1216/high_FPS_video/GOPRO_Large_all/'\n",
    "# train_data_root = '/data/nnice1216/vimeo_septuplet/'\n",
    "train_loader = get_loader('train', train_data_root, args.batch_size, shuffle=True, num_workers=args.num_workers, random_seed=random_seed, seted_interval=interval)\n",
    "# train_loader = get_loader('train', train_data_root, args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "\n",
    "\n",
    "##### Model #####\n",
    "## LIIF ###\n",
    "model_args = {'encoder_spec': {'name': 'edsr-baseline', 'args': {'no_upsampling': True}}, 'imnet_spec': {'name': 'mlp', 'args': {'out_dim': 3, 'hidden_list': [64, 64]}}}\n",
    "model_spec = {'name': 'liif_gtoptical', 'args': model_args}\n",
    "model = models.make(model_spec).to(device)\n",
    "model = nn.DataParallel(model, device_ids=device_ids)\n",
    "# for k, v in model.named_parameters():\n",
    "#     if k[:14] == 'module.encoder':\n",
    "#         v.requires_grad=False\n",
    "        \n",
    "# model.load_state_dict(torch.load('/model/nnice1216/video/FLAVR_2x.pth')['state_dict'], strict=False)\n",
    "\n",
    "if args.if_continue:\n",
    "    name = 'temp-153.pth'\n",
    "#     name = 'vimeo_epoch1_iter499.pth'\n",
    "    print('Load model ' + name)\n",
    "    model.load_state_dict(torch.load('/model/nnice1216/video/' + name))\n",
    "\n",
    "##### Loss & Optimizer #####\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "\n",
    "##### Training #####\n",
    "model.train()\n",
    "loss_f = PerceptualLoss(nn.MSELoss(reduce=True))\n",
    "index = 0\n",
    "\n",
    "args1 = get_raft_args()\n",
    "raft_model = RAFT(args1).to(device)\n",
    "raft_model = nn.DataParallel(raft_model, device_ids=device_ids)\n",
    "raft_model.load_state_dict(torch.load(args1.model))\n",
    "raft_model.eval()\n",
    "        \n",
    "        \n",
    "for epoch_id in range(args.epoch_num):\n",
    "    \n",
    "    print('Epoch {} Begin'.format(epoch_id))\n",
    "    lr = adjust_learning_rate(epoch_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    out_dir = '/output/models/'\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    out_dir2 = '/output/tempimgs22/'\n",
    "    if not os.path.exists(out_dir2):\n",
    "        os.makedirs(out_dir2)\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        ## VIMEO PRE_PROCESS FOR LIIF_3D ##\n",
    "        # images, gt_image, coords, cells, times = data\n",
    "        images, gt_image, coords, _, _ = data\n",
    "        images = [img_.to(device) for img_ in images]\n",
    "        images = torch.stack(images, dim=2)\n",
    "        gt = [g_.to(device) for g_ in gt_image]\n",
    "        # gt = [g_.view(args.batch_size, 3, -1).permute(0, 2, 1).to(device) for g_ in gt_image]\n",
    "        # gt = torch.cat(gt).view(args.batch_size, 3, -1).permute(0, 2, 1)\n",
    "        coords = [c_.to(device) for c_ in coords]\n",
    "        # coord, cell = coord.to(device), cell.to(device)\n",
    " \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        # pred_inter, pred_0, pred_1 = model(img, coord, cell)\n",
    "        # bp()\n",
    "        \n",
    "        preds_f, masks, flows = model(images, coords, True, gts=gt)\n",
    "        \n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        loss4 = 0\n",
    "        \n",
    "        for idx in range(3):\n",
    "#             diff = torch.abs(warps[idx] - convert_to_gray(gt[idx]))\n",
    "#             diff = (diff - diff.min()) / (diff.max() - diff.min())\n",
    "            loss1 += F.smooth_l1_loss(preds_f[idx], gt[idx])\n",
    "            loss4 += F.smooth_l1_loss(masks[idx], flows[idx])\n",
    "            # loss1 += (F.smooth_l1_loss(preds_f[idx], gt[idx], reduce=False) * (att_map1[idx].unsqueeze(1) + 1)).mean()\n",
    "            loss2 += loss_f.get_loss(preds_f[idx], gt[idx]) * 0.05\n",
    "            # loss3 += loss_f.get_loss(preds_f[idx] * ((att_map1[idx].unsqueeze(1)) / 2), gt[idx] * ((att_map1[idx].unsqueeze(1)) / 2)) * 0.05\n",
    "        \n",
    "        # Backward & Update\n",
    "        loss = loss1 + loss2 + loss3 + loss4\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch %d, Iter %d, Loss: %.4f' % (epoch_id, i, loss.item()))\n",
    "        print('Loss1: %.4f, Loss2: %.4f, Loss4: %.4f' % (loss1.item(), loss2.item(), loss4.item()))\n",
    "        # print('Loss1: %.4f, Loss2: %.4f, Loss3: %.4f' % (loss1.item(), loss2.item(), loss3.item()))\n",
    "        # print('Epoch %d, Iter %d, Loss1: %.4f, Loss2: %.4f, Loss: %.4f' % (epoch_id, i, loss1.item(), loss2.item(), loss.item()))\n",
    "        writer.add_scalar('Training Loss', loss.item(), index)\n",
    "        writer.add_scalar('L1 Loss', loss1.item(), index)\n",
    "        # writer.add_scalar('Perceptual Loss', loss2.item(), index)\n",
    "        index += 1\n",
    "        # index += 1\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "            Image.fromarray((gt_image[0][0].permute(1, 2, 0).numpy() * 255).astype(np.uint8)).save(os.path.join(out_dir2, 'Epoch{}_iter{}_GT.jpg'.format(epoch_id, i)))\n",
    "            Image.fromarray((preds_f[0][0].permute(1, 2, 0).clamp(0, 1).detach().cpu().numpy() * 255).astype(np.uint8)).save(os.path.join(out_dir2, 'Epoch{}_iter{}_PRED.jpg'.format(epoch_id, i)))\n",
    "            Image.fromarray((flows[0][0, 0].clamp(0, 1).detach().cpu().numpy() * 255).astype(np.uint8)).save(os.path.join(out_dir2, 'Epoch{}_iter{}_MASKGT.jpg'.format(epoch_id, i)))\n",
    "            Image.fromarray((masks[0][0, 0].clamp(0, 1).detach().cpu().numpy() * 255).astype(np.uint8)).save(os.path.join(out_dir2, 'Epoch{}_iter{}_MASK.jpg'.format(epoch_id, i)))\n",
    "#             Image.fromarray((masks[0][0][0].clamp(0, 1).detach().cpu().numpy() * 255).astype(np.uint8)).save(os.path.join(out_dir2, 'Epoch{}_iter{}_MASK.jpg'.format(epoch_id, i)))\n",
    "#             diff = torch.abs(warps[idx] - convert_to_gray(gt[idx]))\n",
    "#             diff = (diff - diff.min()) / (diff.max() - diff.min())\n",
    "#             img = (diff.clamp(0, 1).detach().cpu() * 255)[0, 0].numpy().astype(np.uint8)\n",
    "#             Image.fromarray(img).save(os.path.join(out_dir2, 'Epoch{}_iter{}_MASK_GT.jpg'.format(epoch_id, i)))\n",
    "            \n",
    "        # if i == 50:\n",
    "        #     test_metric(model, epoch_id, i, True)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            test_images_outp(model, device, 1 / 64, epoch_id, i)\n",
    "        if i % 100 == 10:\n",
    "            test_images_outp(model, device, 11 / 64, epoch_id, i)\n",
    "        if i % 100 == 20:\n",
    "            test_images_outp(model, device, 23 / 64, epoch_id, i)\n",
    "        elif i % 100 == 40:\n",
    "            test_images_outp(model, device, 32 / 64, epoch_id, i)\n",
    "        elif i % 100 == 60:\n",
    "            test_images_outp(model, device, 43 / 64, epoch_id, i)\n",
    "        elif i % 100 == 80:\n",
    "            test_images_outp(model, device, 52 / 64, epoch_id, i)\n",
    "        elif i % 100 == 90:\n",
    "            test_images_outp(model, device, 63 / 64, epoch_id, i)\n",
    "            # out_path = '/output/Image22s/Epoch_{}'.format(epoch_id)\n",
    "            # Image.open('/data/nnice1216/vimeo_septuplet/DAVIS/JPEGImages/Full-Resolution/bmx-rider/00003.jpg').resize((720, 416)).save(os.path.join(out_path, 'Iter{}.jpg'.format(i)))\n",
    "            # test_images_outp(model, device, 7 / 8, epoch_id, i)\n",
    "            # test_images_3d(model, device, 7 / 8, epoch_id, i)\n",
    "            \n",
    "        model.train()\n",
    "        # double_forward(model, optimizer, preds_f, images, gt, device, i, epoch_id, args.batch_size)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            torch.save(model.state_dict(), '/output/models/vimeo_epoch{}_iter{}.pth'.format(epoch_id, i))\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            test_metric(model, epoch_id, i)\n",
    "    # if epoch_id % 7 == 6:\n",
    "    #     test_metric(model, epoch_id, i)\n",
    "    print(\"Epoch {} Done. Index={}\".format(epoch_id, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unparsed args: ['-f', '/root/.local/share/jupyter/runtime/kernel-5bfba0ba-1fae-4d56-974e-e8509906b21c.json']\n",
      "1500\n",
      "Dataset Prepared!\n",
      "BATCHNORM:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/375 [00:39<4:09:13, 39.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 31.812287, SSIM: 0.935985\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/code/FLAVR-main/models/liif_bidi.py\", line 315, in forward\n    return self.query_rgb(coord, Training, index)\n  File \"/code/FLAVR-main/models/liif_bidi.py\", line 228, in query_rgb\n    pred = self.out_conv(self.encode_imnet(encode_inp.view(bs * qs, -1)).view(bs, qs, 64).permute(0, 2, 1).view(bs, 64, feat.shape[2], feat.shape[3]))\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/code/FLAVR-main/models/SIREN.py\", line 79, in forward\n    output = self.net(coords)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 92, in forward\n    input = module(input)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/code/FLAVR-main/models/SIREN.py\", line 46, in forward\n    return torch.sin(self.omega_0 * self.linear(input))\nRuntimeError: CUDA out of memory. Tried to allocate 900.00 MiB (GPU 0; 10.92 GiB total capacity; 8.36 GiB already allocated; 463.56 MiB free; 1.55 GiB cached)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1287d5a9fabe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-1287d5a9fabe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1287d5a9fabe>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# out = model(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/code/FLAVR-main/models/liif_bidi.py\", line 315, in forward\n    return self.query_rgb(coord, Training, index)\n  File \"/code/FLAVR-main/models/liif_bidi.py\", line 228, in query_rgb\n    pred = self.out_conv(self.encode_imnet(encode_inp.view(bs * qs, -1)).view(bs, qs, 64).permute(0, 2, 1).view(bs, 64, feat.shape[2], feat.shape[3]))\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/code/FLAVR-main/models/SIREN.py\", line 79, in forward\n    output = self.net(coords)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 92, in forward\n    input = module(input)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/code/FLAVR-main/models/SIREN.py\", line 46, in forward\n    return torch.sin(self.omega_0 * self.linear(input))\nRuntimeError: CUDA out of memory. Tried to allocate 900.00 MiB (GPU 0; 10.92 GiB total capacity; 8.36 GiB already allocated; 463.56 MiB free; 1.55 GiB cached)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import myutils\n",
    "import models\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pdb import set_trace as bp\n",
    "from PIL import Image\n",
    "\n",
    "from gpu_memory_log import gpu_memory_log\n",
    "\n",
    "##### Parse CmdLine Arguments #####\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]='7'\n",
    "args, unparsed = config.get_args()\n",
    "cwd = os.getcwd()\n",
    "\n",
    "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
    "device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "torch.manual_seed(args.random_seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.random_seed)\n",
    "\n",
    "if args.dataset == \"vimeo90K_septuplet\":\n",
    "    from dataset.vimeo90k_septuplet import get_loader\n",
    "    test_loader = get_loader('test', args.data_root, args.test_batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "elif args.dataset == \"ucf101\":\n",
    "    from dataset.ucf101_test import get_loader\n",
    "    test_loader = get_loader(args.data_root, args.test_batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "elif args.dataset == \"gopro\":\n",
    "    from dataset.GoPro import get_loader\n",
    "    test_loader = get_loader('train', '/data/nnice1216/high_FPS_video/GOPRO_Large_all/', 1, shuffle=False, num_workers=8, interFrames=7)  \n",
    "    print(\"Dataset Prepared!\")\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "##### FLAVR #####\n",
    "'''\n",
    "from model.FLAVR_arch import UNet_3D_3D\n",
    "print(\"Building model: %s\"%args.model.lower())\n",
    "model = UNet_3D_3D(args.model.lower() , n_inputs=args.nbr_frame, n_outputs=args.n_outputs, joinType=args.joinType)\n",
    "\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "model_dict = model.state_dict()\n",
    "model.load_state_dict(torch.load(args.load_from)[\"state_dict\"] , strict=True)\n",
    "print(\"#params\" , sum([p.numel() for p in model.parameters()]))\n",
    "'''\n",
    "##### LIIF #####\n",
    "model_args = {'encoder_spec': {'name': 'edsr-baseline', 'args': {'no_upsampling': True}}, 'imnet_spec': {'name': 'mlp', 'args': {'out_dim': 3, 'hidden_list': [64, 64]}}}\n",
    "model_spec = {'name': 'liif_bidi', 'args': model_args}\n",
    "model = models.make(model_spec).to(device)\n",
    "model = nn.DataParallel(model, device_ids=device_ids)\n",
    "name = 'temp-739.pth'\n",
    "model.load_state_dict(torch.load('/model/nnice1216/video/' + name))\n",
    "\n",
    "\n",
    "def test(args):\n",
    "    time_taken = []\n",
    "    losses, psnrs, ssims = myutils.init_meters(args.loss)\n",
    "    model.eval()\n",
    "    out_dir = \"/output/gopro_test/\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    psnr_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, gt_image, coords, cells, times) in enumerate(tqdm(test_loader)):\n",
    "            # if i > 100:\n",
    "            #     break\n",
    "            images = [img_[None].cuda() for img_ in images]\n",
    "            images = torch.cat(images, dim=0).permute(1, 2, 0, 3, 4)\n",
    "            gt = [g_.cuda() for g_ in gt_image]\n",
    "            coords = [c_.cuda() for c_ in coords]\n",
    "            cells = [c_.cuda() for c_ in cells]\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            # out = model(images)\n",
    "        \n",
    "            out = model(images, coords, \"testing\")\n",
    "            \n",
    "            out = torch.cat(out)\n",
    "            gt = torch.cat(gt)\n",
    "            '''\n",
    "            for j in range(7):\n",
    "                Image.fromarray((gt[j].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)).save(os.path.join(out_dir, 'Epoch{}_iter{}_GT.jpg'.format(i, j)))\n",
    "                Image.fromarray((out[j].permute(1, 2, 0).clamp(0, 1).detach().cpu().numpy() * 255).astype(np.uint8)).save(os.path.join(out_dir, 'Epoch{}_iter{}_PRED.jpg'.format(i, j)))\n",
    "            '''\n",
    "            torch.cuda.synchronize()\n",
    "            time_taken.append(time.time() - start_time)\n",
    "            myutils.eval_metrics(out, gt, psnrs, ssims)\n",
    "\n",
    "            print(\"PSNR: %f, SSIM: %f\" %\n",
    "          (psnrs.avg, ssims.avg))\n",
    "    print(\"FINAL: PSNR: %f, SSIM: %f\" %\n",
    "          (psnrs.avg, ssims.avg))\n",
    "    print(\"Time: \" , sum(time_taken)/len(time_taken))\n",
    "\n",
    "    return psnrs.avg\n",
    "\n",
    "\n",
    "\"\"\" Entry Point \"\"\"\n",
    "def main(args):\n",
    "    \n",
    "    assert args.load_from is not None\n",
    "\n",
    "    test(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "with open('dataset/GoPro_test.txt') as t:\n",
    "    file = t.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
